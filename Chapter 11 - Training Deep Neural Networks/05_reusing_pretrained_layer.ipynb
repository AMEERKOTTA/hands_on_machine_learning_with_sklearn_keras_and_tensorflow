{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REUSING PRETRAINED LAYERS\n",
    "\n",
    "+ In Practice we cannot train deep neural networks from scratch.\n",
    "+ Instead we can make use of the Existing Neural Networks that accomplishes a similar task.\n",
    "+ Like this Reusing the Lower Layer for solving another Problems is Called Transfer Learning.\n",
    "\n",
    "*ADVANTAGES OF TRANSFER LEARNING*\n",
    "\n",
    "+ It will speed the Training.\n",
    "+ It require only less training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transfer Learning with Keras**\n",
    "\n",
    "+ Consider we have a model for Image Classification for MNIST Fashion dataset for 8 Classes.\n",
    "+ Which is model A.\n",
    "+ Now I need to build a Binary Classifier for Sandal and Shirt.\n",
    "+ Your Dataset is Quite Small.\n",
    "+ So we will take advantage of the lower layer of Model A to train and Develop the Binary Classifier.\n",
    "+ split the data into two \n",
    "+ X_train_A =  all images of all items except for sandals and shirts (classes 5 and 6).\n",
    "+ X_train_B = a much smaller training set of just the first 200 images of sandals or shirts.\n",
    "+ The validation set and the test set are also split this way, but without restricting the number of images.\n",
    "+ We will train a model on set A (classification task with 8 classes)\n",
    "+ and try to reuse it to tackle set B (binary classification)\n",
    "+ In the first `model A`, `we have an accuracy: 0.9255 and val_accuracy: 0.9228`\n",
    "\n",
    "+ In the similar fashion, we trained the MODEL A.\n",
    "+ `accuracy: 1.0000, val_accuracy: 0.9848`\n",
    "\n",
    "+ Doing the Transfer Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43986, 28, 28)\n",
      "(200, 28, 28)\n",
      "(43986,)\n",
      "(200,)\n",
      "(4014, 28, 28)\n",
      "(986, 28, 28)\n",
      "(8000, 28, 28)\n",
      "(2000, 28, 28)\n",
      "[4 0 5 7 7 7 4 4 3 4 0 1 6 3 4 3 2 6 5 3 4 5 1 3 4 2 0 6 7 1]\n",
      "[1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.\n",
      " 1. 0. 1. 1. 1. 1.]\n",
      "===================BUILDING THE MODEL==========================\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_4 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 8)                 408       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 276,158\n",
      "Trainable params: 276,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "===================COMPILE THE MODEL==========================\n",
      "====================TRAIN THE MODEL===========================\n",
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 6s 3ms/step - loss: 0.6360 - accuracy: 0.7916 - val_loss: 0.3953 - val_accuracy: 0.8719\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 3s 3ms/step - loss: 0.3670 - accuracy: 0.8736 - val_loss: 0.3312 - val_accuracy: 0.8879\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.3244 - accuracy: 0.8874 - val_loss: 0.3088 - val_accuracy: 0.8946\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.3027 - accuracy: 0.8950 - val_loss: 0.2890 - val_accuracy: 0.9048\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2883 - accuracy: 0.9008 - val_loss: 0.2802 - val_accuracy: 0.9088\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 3s 3ms/step - loss: 0.2774 - accuracy: 0.9039 - val_loss: 0.2705 - val_accuracy: 0.9103\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2689 - accuracy: 0.9068 - val_loss: 0.2667 - val_accuracy: 0.9116\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 3s 3ms/step - loss: 0.2626 - accuracy: 0.9103 - val_loss: 0.2595 - val_accuracy: 0.9150\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 3s 3ms/step - loss: 0.2568 - accuracy: 0.9113 - val_loss: 0.2582 - val_accuracy: 0.9143\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2515 - accuracy: 0.9138 - val_loss: 0.2539 - val_accuracy: 0.9168\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 3s 3ms/step - loss: 0.2472 - accuracy: 0.9141 - val_loss: 0.2487 - val_accuracy: 0.9165\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 3s 3ms/step - loss: 0.2431 - accuracy: 0.9161 - val_loss: 0.2511 - val_accuracy: 0.9163\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2393 - accuracy: 0.9175 - val_loss: 0.2491 - val_accuracy: 0.9185\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2358 - accuracy: 0.9201 - val_loss: 0.2456 - val_accuracy: 0.9163\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2325 - accuracy: 0.9207 - val_loss: 0.2435 - val_accuracy: 0.9168\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2294 - accuracy: 0.9218 - val_loss: 0.2386 - val_accuracy: 0.9178\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2268 - accuracy: 0.9220 - val_loss: 0.2468 - val_accuracy: 0.9160\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2241 - accuracy: 0.9230 - val_loss: 0.2363 - val_accuracy: 0.9175\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2217 - accuracy: 0.9226 - val_loss: 0.2356 - val_accuracy: 0.9200\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2197 - accuracy: 0.9248 - val_loss: 0.2318 - val_accuracy: 0.9188\n",
      "=====================SAVE THE MODEL==========================\n"
     ]
    }
   ],
   "source": [
    "## fetch the mnist dataset from the library\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "## split the data into two \n",
    "## X_train_A =  all images of all items except for sandals and shirts (classes 5 and 6).\n",
    "## X_train_B = a much smaller training set of just the first 200 images of sandals or shirts.\n",
    "## The validation set and the test set are also split this way, but without restricting the number of images.\n",
    "## We will train a model on set A (classification task with 8 classes)\n",
    "## and try to reuse it to tackle set B (binary classification)\n",
    "\n",
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]\n",
    "\n",
    "print(X_train_A.shape)\n",
    "print(X_train_B.shape)\n",
    "print(y_train_A.shape)\n",
    "print(y_train_B.shape)\n",
    "print(X_valid_A.shape)\n",
    "print(X_valid_B.shape)\n",
    "print(X_test_A.shape)\n",
    "print(X_test_B.shape)\n",
    "\n",
    "print(y_train_A[:30])\n",
    "print(y_train_B[:30])\n",
    "\n",
    "print(\"===================BUILDING THE MODEL==========================\")\n",
    "## define the model A\n",
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "## classifying the 8 classes.\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))\n",
    "print(model_A.summary())\n",
    "\n",
    "print(\"===================COMPILE THE MODEL==========================\")\n",
    "## sparse categorical cross entropy is becuase we have 8 classes to classify.\n",
    "## optimizer is used as SGD stochastic gradient descent.\n",
    "## metrics is accuracy\n",
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"====================TRAIN THE MODEL===========================\")\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,validation_data=(X_valid_A, y_valid_A))\n",
    "\n",
    "print(\"=====================SAVE THE MODEL==========================\")\n",
    "model_A.save(\"model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================BUILDING THE MODEL==========================\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 8)                 408       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 276,158\n",
      "Trainable params: 276,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "===================COMPILE THE MODEL==========================\n",
      "====================TRAIN THE MODEL===========================\n",
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 45ms/step - loss: 1.8515 - accuracy: 0.3500 - val_loss: 0.8988 - val_accuracy: 0.7272\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.7444 - accuracy: 0.8000 - val_loss: 0.5640 - val_accuracy: 0.8813\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.4764 - accuracy: 0.9050 - val_loss: 0.4081 - val_accuracy: 0.9320\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.3438 - accuracy: 0.9650 - val_loss: 0.3251 - val_accuracy: 0.9533\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.2708 - accuracy: 0.9850 - val_loss: 0.2716 - val_accuracy: 0.9665\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.2220 - accuracy: 0.9900 - val_loss: 0.2323 - val_accuracy: 0.9726\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.1856 - accuracy: 0.9950 - val_loss: 0.2058 - val_accuracy: 0.9726\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.1596 - accuracy: 1.0000 - val_loss: 0.1832 - val_accuracy: 0.9767\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.1399 - accuracy: 1.0000 - val_loss: 0.1682 - val_accuracy: 0.9787\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.1255 - accuracy: 1.0000 - val_loss: 0.1552 - val_accuracy: 0.9807\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.1134 - accuracy: 1.0000 - val_loss: 0.1435 - val_accuracy: 0.9807\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.1022 - accuracy: 1.0000 - val_loss: 0.1337 - val_accuracy: 0.9807\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0931 - accuracy: 1.0000 - val_loss: 0.1262 - val_accuracy: 0.9807\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0860 - accuracy: 1.0000 - val_loss: 0.1204 - val_accuracy: 0.9817\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0802 - accuracy: 1.0000 - val_loss: 0.1145 - val_accuracy: 0.9817\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0748 - accuracy: 1.0000 - val_loss: 0.1095 - val_accuracy: 0.9828\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0701 - accuracy: 1.0000 - val_loss: 0.1053 - val_accuracy: 0.9828\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0660 - accuracy: 1.0000 - val_loss: 0.1016 - val_accuracy: 0.9838\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0621 - accuracy: 1.0000 - val_loss: 0.0978 - val_accuracy: 0.9838\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0586 - accuracy: 1.0000 - val_loss: 0.0945 - val_accuracy: 0.9848\n"
     ]
    }
   ],
   "source": [
    "## same way I'm training the MODEL B.\n",
    "## check the model perfomance\n",
    "print(\"===================BUILDING THE MODEL==========================\")\n",
    "## define the model A\n",
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "## classifying the 8 classes.\n",
    "model_B.add(keras.layers.Dense(8, activation=\"softmax\"))\n",
    "print(model_B.summary())\n",
    "\n",
    "print(\"===================COMPILE THE MODEL==========================\")\n",
    "## sparse categorical cross entropy is becuase we have 8 classes to classify.\n",
    "## optimizer is used as SGD stochastic gradient descent.\n",
    "## metrics is accuracy\n",
    "model_B.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"====================TRAIN THE MODEL===========================\")\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRANSFER LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================MODEL DEVELOPMENT========================\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_4 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 275,801\n",
      "Trainable params: 275,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "======================TAKING THE CLONE OF THE MODEL==========================\n",
      "=======================COMPILE THE MODEL=======================\n",
      "========================TRAIN THE MODEL FOR 4 EPOCHS=======================\n",
      "Epoch 1/4\n",
      "7/7 [==============================] - 1s 40ms/step - loss: 0.5253 - accuracy: 0.7150 - val_loss: 0.5375 - val_accuracy: 0.6927\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.4967 - accuracy: 0.7450 - val_loss: 0.5097 - val_accuracy: 0.7191\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.4697 - accuracy: 0.7550 - val_loss: 0.4846 - val_accuracy: 0.7434\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.4454 - accuracy: 0.7800 - val_loss: 0.4610 - val_accuracy: 0.7606\n",
      "======================COMPILE THE MODE AGAIN AFTER UNFREEZING THE RESUED LAYERS=====================\n",
      "==============TRAIN THE MODEL AGAIN=====================\n",
      "Epoch 1/16\n",
      "7/7 [==============================] - 1s 39ms/step - loss: 0.4250 - accuracy: 0.7900 - val_loss: 0.4460 - val_accuracy: 0.7728\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.4096 - accuracy: 0.8050 - val_loss: 0.4315 - val_accuracy: 0.7830\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.3949 - accuracy: 0.8250 - val_loss: 0.4182 - val_accuracy: 0.7941\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.3815 - accuracy: 0.8350 - val_loss: 0.4053 - val_accuracy: 0.8012\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.3685 - accuracy: 0.8400 - val_loss: 0.3928 - val_accuracy: 0.8144\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.3563 - accuracy: 0.8650 - val_loss: 0.3812 - val_accuracy: 0.8245\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.3446 - accuracy: 0.8650 - val_loss: 0.3698 - val_accuracy: 0.8418\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.3331 - accuracy: 0.8650 - val_loss: 0.3588 - val_accuracy: 0.8519\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.3221 - accuracy: 0.8950 - val_loss: 0.3488 - val_accuracy: 0.8611\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.3121 - accuracy: 0.9050 - val_loss: 0.3389 - val_accuracy: 0.8671\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.3023 - accuracy: 0.9200 - val_loss: 0.3295 - val_accuracy: 0.8753\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.2930 - accuracy: 0.9200 - val_loss: 0.3206 - val_accuracy: 0.8844\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.2841 - accuracy: 0.9300 - val_loss: 0.3120 - val_accuracy: 0.8915\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.2757 - accuracy: 0.9350 - val_loss: 0.3038 - val_accuracy: 0.8955\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.2677 - accuracy: 0.9400 - val_loss: 0.2956 - val_accuracy: 0.9006\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.2594 - accuracy: 0.9550 - val_loss: 0.2885 - val_accuracy: 0.9037\n"
     ]
    }
   ],
   "source": [
    "print(\"=======================MODEL DEVELOPMENT========================\")\n",
    "## fetch the saved model.\n",
    "model_A = keras.models.load_model(\"model_A.h5\")\n",
    "## created the model B on A\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "## adding the last layer for binary classifier.\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "print(model_B_on_A.summary())\n",
    "print(\"======================TAKING THE CLONE OF THE MODEL==========================\")\n",
    "## since the model A and model B on A, both are sharing the layers.\n",
    "## we need to clone the layers of A.\n",
    "## other while training the second model.\n",
    "## both will modify.\n",
    "## take the clone of A\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "## then set the weights at that time.\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "## now define the model B on A on top of the A clone\n",
    "model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "## adding the last layer\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "## when the shared layer got trained again.\n",
    "## there will be large error.\n",
    "## so freeze those layers to avoid these issues.\n",
    "## and give time to learn for the new layers.\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "## we need to compile the model after freeze or unfreeze the layers    \n",
    "print(\"=======================COMPILE THE MODEL=======================\")\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "print(\"========================TRAIN THE MODEL FOR 4 EPOCHS=======================\")\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "## now unfreeze the reused layer\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "print(\"======================COMPILE THE MODE AGAIN AFTER UNFREEZING THE RESUED LAYERS=====================\")\n",
    "## reduce the learning rate.\n",
    "optimizer = keras.optimizers.SGD(learning_rate = 1e-4)\n",
    "## compile the model again\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=optimizer,\n",
    "                     metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"==============TRAIN THE MODEL AGAIN=====================\")\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0968 - accuracy: 0.9810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09681594371795654, 0.9810000061988831]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## checking the output\n",
    "model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2729 - accuracy: 0.9130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2728533446788788, 0.9129999876022339]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no Improvemment I can See on the Transfer Learning Model.\n",
    "+ Transfer Learning work best with Deep CNNs, which tend to learn feature detectors that are much more general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNSUPERVISED PRETRAINING\n",
    "\n",
    "+ Suppose you want tackle a complex task.\n",
    "+ But you dont have much labelled training data.\n",
    "+ And you cant find a model trained for similar tasks.\n",
    "+ Then you can use Unsupervised Pretraining.\n",
    "\n",
    "    + gather plenty of unlabelled training data.\n",
    "    + use it to train unsupervised model.\n",
    "    + such as AutoEncoders, GANS etc.\n",
    "    + then you can reuse lower layer of these AutoEncoders and GAN.\n",
    "    + Add the output layer of your new task on top of it.\n",
    "    + Fine tune the network using SuperVised Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRETRAINING ON AUXILIARY TASKS\n",
    "\n",
    "+ If you dont have much labelled training data.\n",
    "+ train a NN on auxiliary tasks, for which you can easily obtain or generate the labelled data.\n",
    "+ then reuse the lower layer of that network to for your actual task.\n",
    "+ first NN lower layer will learn feature detectors.\n",
    "+ then we can reuse it to define the second NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
