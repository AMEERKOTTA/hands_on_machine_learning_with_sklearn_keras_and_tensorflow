{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIMIZATION FUNCTIONS\n",
    "\n",
    "Training very Large Deep Neural Network can be very Slow. So Five Steps to Improve the Speed of the Training.\n",
    "\n",
    "1. Applying a good Initialization Strategy for the Connection Weights.\n",
    "2. Using good Activation Function.\n",
    "3. Using Batch Normalization.\n",
    "4. Reusing Parts of Pretrained Networks.\n",
    "5. Using faster Optimization than Regular Gradient Descent Optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *1. GRADIENT DESCENT*\n",
    "\n",
    "+ Most basic and most used Optimization Algorithm.\n",
    "+ Used heavily in Linear Regression and Classification Algorithms.\n",
    "+ BackPropogation in Neural Networks also uses the Gradient Descent.\n",
    "+ Gradient Descent is a first order Optimization Algorithm.\n",
    "+ It is the first order Dervative of the Loss Function.\n",
    "+ It calculates that in which direction weights should be altered, in order to reach the minima.\n",
    "+ through back propogation, the loss is transfered from one layer to another, and model's parameter also known as Weights.\n",
    "+ weights are modified depends on the losses so that loss can be minimized.\n",
    "\n",
    "ADVANTAGES\n",
    "\n",
    "+ Easy to Compute.\n",
    "+ Easy to Implement.\n",
    "+ Easy to Understand.\n",
    "\n",
    "DISADVANTAGES\n",
    "\n",
    "+ May trap at the local minima.\n",
    "+ Weights are changed, after calculating the gradient on the whole dataset.\n",
    "+ that is one updation in the weights per whole training.\n",
    "+ So if the dataset is large, then this may take a lot of time to converge.\n",
    "+ Requires large memory to Calculate the gradient on the Whole Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2. STOCHASTIC GRADIENT DESCENT ALGORITHM*\n",
    "\n",
    "+ Variant of Gradient Descent.\n",
    "+ It tries to Update the models parameter more frequently.\n",
    "+ In this model parameters are altered after computation of losses on each training example.\n",
    "+ that is for 1000 rows of data, SGD will update parameters 1000 times.\n",
    "+ In gradient descent it will be only once per training 1000 datapoints.\n",
    "+ `As the model parameters are frequently updated, here parameters have high varince and fluctuations in loss functions.`\n",
    "\n",
    "ADVANTAGES\n",
    "\n",
    "+ Converges in less time.\n",
    "+ Requires less memory as no need to store values of loss functions.\n",
    "+ May get the new minima.\n",
    "\n",
    "\n",
    "DISADVANTAGES\n",
    "\n",
    "+ `High Variance in the Model Parameter Values.`\n",
    "+ May shoot even after achieving the global minima.\n",
    "+ To get some convergence as gradient descent, needs to slowy reduce the value of learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *3. MINI-BATCH GRADIENT DESCENT*\n",
    "\n",
    "+ Best among Gradient Algorithms.\n",
    "+ Improvement on SGD and normal Gradient Descent.\n",
    "+ It updates the model parameters after every Batch.\n",
    "+ Here the dataset is divided into batches (mini batches).\n",
    "+ And after every batch parameters are updated.\n",
    "\n",
    "ADVANTAGES\n",
    "\n",
    "+ Frequently updates the Model Parameters.\n",
    "+ And also has less Variance.\n",
    "+ Requires Medium amount of Memory.\n",
    "\n",
    "DISADVANTAGES\n",
    "\n",
    "+ Choosing an Optimal Value of learning rate. If the learning rate is too small.\n",
    "+ then model will only converge very Slowly.\n",
    "+ Here a Constant learning rate for all the Parameters.\n",
    "+ There may be some parameters which we may not want to change at the same rate.\n",
    "+ May get trapped at the Local Minima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *4. MOMENTUM OPTIMIZER*\n",
    "\n",
    "\n",
    "+ In this it will reduce the High Variance in SGD.\n",
    "+ And Softens the Converges.\n",
    "+ It Accelerates the Convergence to the Right Direction.\n",
    "+ Reduces the fluctuations to the Irrelevent Direction.\n",
    "+ One more HP is used in this method, gamma = momentum\n",
    "+ Usual Value gamma = 0.9\n",
    "+ `optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)`\n",
    "\n",
    "ADVANTAGES\n",
    "\n",
    "+ Reduces the Oscillations and High Variance of the Parameters.\n",
    "+ Converges faster than Gradient Descent.\n",
    "\n",
    "\n",
    "DISADVANTAGES\n",
    "\n",
    "+ One more HP is added, which needs to be selected manually and accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *5. NESTEROV ACCELERATED GRADIENT*\n",
    "\n",
    "+ Momentum may be a good method.\n",
    "+ But, if momentum is too high, the algorithm may miss the local minima.\n",
    "+ and continue to rise up.\n",
    "+ It is a Modification to the above Problem.\n",
    "+ `optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)`\n",
    "\n",
    "ADVANTAGES\n",
    "\n",
    "+ Does not miss the local minima\n",
    "+ Slows the process if minima is Occuring.\n",
    "\n",
    "DISADVANTAGES\n",
    "\n",
    "+ Still HP needs to be selected manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *6. ADAGRAD OPTIMIZER*\n",
    "\n",
    "+ learning rate was constant for all the Optimizers explanied before.\n",
    "+ this optimizer changes the learing rate for each parameter.\n",
    "+ It is a Second order Optimization Algorithm.\n",
    "+ It works on the Derivative of Error Function.\n",
    "+ It makes big Updates for less frequent parameter and small updates for frequent parameters.\n",
    "+ `optimizer = keras.optimizers.Adagrad(learning_rate=0.001)`\n",
    "\n",
    "ADVANTAGES\n",
    "\n",
    "+ Learning rate changes for each training parameters.\n",
    "+ Don't need to manually tune Learning rate.\n",
    "+ Able to train on Sparse Data.\n",
    "\n",
    "DISADVANTAGES\n",
    "\n",
    "+ Computationally Expensive, and need to Calculate the Second Order Derivative.\n",
    "+ The Learning rate is always reducing results in slow training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *7. ADADELTA OPTIMIZER*\n",
    "\n",
    "+ Extension of Adagrad.\n",
    "+ Which removes the decaying learning rate problem.\n",
    "\n",
    "ADVANTAGES\n",
    "\n",
    "+ learning rate does not decay, and training does not stop.\n",
    "\n",
    "DISADVANTAGES\n",
    "\n",
    "+ Computationally Expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *8. RMS PROP*\n",
    "\n",
    "+ Adagrad runs the risk of Slowing down.\n",
    "+ It may not Converge to Global Optimum.\n",
    "+ RMS Prop fixing this by taking only the recent gradients.\n",
    "+ the decay rate = beta = typically = 0.9\n",
    "+ `optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *9. ADAM (ADAPTIVE MOMENTUM ESIMATION)*\n",
    "\n",
    "+ It works with Momentum of first and second order.\n",
    "+ The intuition behind adam is that, we dont want to roll up so fast just because we can jump over the minimum.\n",
    "+ We want to decrease the velocity for careful search.\n",
    "+ Adam is the Combination of momentum and rmsprop.\n",
    "+ `optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)`\n",
    "+ beta1 = momentum decay HP\n",
    "+ beta2 = scaling decay HP\n",
    "\n",
    "ADVANTAGES\n",
    "\n",
    "+ this method is too fast.\n",
    "+ it converges rapidly.\n",
    "+ rectifies vanishing learning rates and high variances.\n",
    "\n",
    "\n",
    "DISADVANTAGES\n",
    "\n",
    "+ Computationally Costly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *TWO VARIANTS OF ADAM*\n",
    "\n",
    "ADAMAX\n",
    "\n",
    "+ `optimizer = keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999)`\n",
    "\n",
    "NADAM\n",
    "\n",
    "+ `optimizer = keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
