{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING CALLBACKS AND EARLYSTOPPING\n",
    "\n",
    "+ callbacks are used to save the model checkpoints at regular intervals.\n",
    "+ the fit() method accept callbacka arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================Shape of the Full Training and Test Set======================\n",
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n",
      "===================Shape of the Training and Test Set after the Splitting to Validation Set======================\n",
      "(5000, 28, 28)\n",
      "(55000, 28, 28)\n",
      "(5000,)\n",
      "(55000,)\n",
      "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n",
      "7\n",
      "Coat\n",
      "Sneaker\n",
      "==============Building the Model using Sequntial API=======================\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[<keras.layers.reshaping.flatten.Flatten object at 0x0000022C29522A30>, <keras.layers.core.dense.Dense object at 0x0000022C28EBFBE0>, <keras.layers.core.dense.Dense object at 0x0000022C28FF32B0>, <keras.layers.core.dense.Dense object at 0x0000022C2952C910>]\n",
      "dense_3\n",
      "[[ 0.07414743  0.05667996 -0.0670316  ... -0.01940319  0.02761105\n",
      "  -0.06365021]\n",
      " [-0.0642621  -0.05441209  0.06555285 ...  0.06959949  0.03537221\n",
      "   0.05710886]\n",
      " [-0.07394297  0.02759565  0.04339147 ... -0.02049001 -0.06565505\n",
      "  -0.01072721]\n",
      " ...\n",
      " [-0.04544195  0.02095098  0.03923605 ...  0.0288021  -0.0160833\n",
      "  -0.07228786]\n",
      " [ 0.07316217 -0.01497868 -0.00019722 ...  0.00841518 -0.05926242\n",
      "  -0.01768987]\n",
      " [ 0.05903645  0.00739175 -0.00162394 ... -0.0272452  -0.00053707\n",
      "   0.00759625]]\n",
      "(784, 300)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(300,)\n",
      "=================Compiling the model=====================\n",
      "================Use Callbacks============================\n",
      "================Training the Model=======================\n",
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.7252 - accuracy: 0.7654\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4925 - accuracy: 0.8293\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4484 - accuracy: 0.8431\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4196 - accuracy: 0.8526\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3984 - accuracy: 0.8603\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3823 - accuracy: 0.8659\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3687 - accuracy: 0.8709\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3568 - accuracy: 0.8740\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3455 - accuracy: 0.8777\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3360 - accuracy: 0.8806\n"
     ]
    }
   ],
   "source": [
    "## use keras to load the dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "print(\"===================Shape of the Full Training and Test Set======================\")\n",
    "print(X_train_full.shape)\n",
    "print(y_train_full.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "## the dataset is already splitted to training and test set.\n",
    "## now the data is to be splitted to validation set.\n",
    "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "print(\"===================Shape of the Training and Test Set after the Splitting to Validation Set======================\")\n",
    "print(X_valid.shape)\n",
    "print(X_train.shape)\n",
    "print(y_valid.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "## for fashion mnist, the class names are\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]\n",
    "print(class_names)\n",
    "\n",
    "## first example in the trainning set \n",
    "print(y_train[2])\n",
    "print(class_names[y_train[0]])\n",
    "print(class_names[y_train[2]])\n",
    "\n",
    "print(\"==============Building the Model using Sequntial API=======================\")\n",
    "##defining the model\n",
    "## this is the line to reate the sequential model\n",
    "## simplest kind of keras model for neural networks, this is called sequential API.\n",
    "model = keras.models.Sequential()\n",
    "## First Layer is Flatten Layer\n",
    "## This will convert each input image to 1D array.\n",
    "## here we have to specify the input shape  = (28,28)\n",
    "## we dont need to give the batch size here. only the input shape.\n",
    "## alternatively we can give keras.layers.InputLayer(input_shape=[28,28])\n",
    "model.add(keras.layers.Flatten(input_shape = [28,28]))\n",
    "## next layer is dense layer with 300 neurons.\n",
    "## it will use ReLU activation function.\n",
    "model.add(keras.layers.Dense(300, activation = \"relu\"))\n",
    "## next layer is also a Dense layer with 100 neurons\n",
    "## it also uses the ReLU activation function.\n",
    "model.add(keras.layers.Dense(100, activation = \"relu\"))\n",
    "## next layer is the final layer with 10 neurons. to classify the 10 classes.\n",
    "## here using the softmax actiavtion function.\n",
    "model.add(keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "# ## we can do the same thing in the given below manner as well\n",
    "# model = keras.models.Sequential([\n",
    "#     keras.layers.Flatten(input_shape=[28,28]),\n",
    "#     keras.layers.Dense(300, activation = \"relu\"),\n",
    "#     keras.layers.Dense(100, activation = \"relu\"),\n",
    "#     keras.layers.Dense(10, activation = \"softmax\")\n",
    "# ])\n",
    "\n",
    "## printing the model summary\n",
    "print(model.summary())\n",
    "## print the model layers\n",
    "print(model.layers)\n",
    "## check the first hidden layer\n",
    "hidden1 = model.layers[1]\n",
    "print(hidden1.name)\n",
    "## checking the weights and biases.\n",
    "weights, biases = hidden1.get_weights()\n",
    "print(weights)\n",
    "print(weights.shape)\n",
    "print(biases)\n",
    "print(biases.shape)\n",
    "\n",
    "print(\"=================Compiling the model=====================\")\n",
    "## compile the model\n",
    "## compile is to specify the loss fnuction and optimizer to use while training and evaluating.\n",
    "## also we can give the metrics to use while training and evaluating model.\n",
    "## using loss = \"sparse_categorical_crossentropy\"\n",
    "## using optimizer as \"sgd\" which is stochcastic gradient descent.\n",
    "## using the metrics as \"accuracy\"\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "                 optimizer = \"sgd\",\n",
    "                 metrics = [\"accuracy\"])\n",
    "\n",
    "print(\"================Use Callbacks============================\")\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"fashion_mnist_classifier_model.h5\")\n",
    "print(\"================Training the Model=======================\")\n",
    "history = model.fit(X_train, y_train, epochs = 10, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## when you are using the save_best_only = True\n",
    "## In this it will only save you model when its perfomance on the validation set is best so far.\n",
    "## this way you dont need to worry about training for very long time.\n",
    "## And you dont need to think about overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================Shape of the Full Training and Test Set======================\n",
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n",
      "===================Shape of the Training and Test Set after the Splitting to Validation Set======================\n",
      "(5000, 28, 28)\n",
      "(55000, 28, 28)\n",
      "(5000,)\n",
      "(55000,)\n",
      "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n",
      "7\n",
      "Coat\n",
      "Sneaker\n",
      "==============Building the Model using Sequntial API=======================\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_3 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[<keras.layers.reshaping.flatten.Flatten object at 0x0000022C2891A670>, <keras.layers.core.dense.Dense object at 0x0000022C2921A5B0>, <keras.layers.core.dense.Dense object at 0x0000022C29522670>, <keras.layers.core.dense.Dense object at 0x0000022C2921A940>]\n",
      "dense_9\n",
      "[[-0.006749   -0.01509491 -0.05785394 ...  0.03529102  0.07341969\n",
      "  -0.03740548]\n",
      " [-0.07164929 -0.02243688  0.04138477 ... -0.04140224 -0.0421823\n",
      "  -0.07216419]\n",
      " [ 0.01395563  0.02569757 -0.07390098 ...  0.05840868 -0.026317\n",
      "   0.00432049]\n",
      " ...\n",
      " [-0.04852203  0.01747886  0.0346866  ... -0.02716882 -0.06463016\n",
      "  -0.06827543]\n",
      " [ 0.05132557  0.04194527 -0.05556519 ... -0.02811012  0.05495334\n",
      "   0.04808994]\n",
      " [-0.05321608 -0.04060496  0.02303836 ...  0.04254295 -0.06783491\n",
      "   0.04747322]]\n",
      "(784, 300)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(300,)\n",
      "=================Compiling the model=====================\n",
      "================Use Callbacks============================\n",
      "================Training the Model=======================\n",
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.7353 - accuracy: 0.7564 - val_loss: 0.5243 - val_accuracy: 0.8248\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4916 - accuracy: 0.8286 - val_loss: 0.4450 - val_accuracy: 0.8468\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.4441 - accuracy: 0.8442 - val_loss: 0.4245 - val_accuracy: 0.8528\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4163 - accuracy: 0.8543 - val_loss: 0.3996 - val_accuracy: 0.8628\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3950 - accuracy: 0.8599 - val_loss: 0.4220 - val_accuracy: 0.8488\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3796 - accuracy: 0.8655 - val_loss: 0.3661 - val_accuracy: 0.8758\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3645 - accuracy: 0.8714 - val_loss: 0.3611 - val_accuracy: 0.8772\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3533 - accuracy: 0.8749 - val_loss: 0.3499 - val_accuracy: 0.8796\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3434 - accuracy: 0.8773 - val_loss: 0.3459 - val_accuracy: 0.8812\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3335 - accuracy: 0.8818 - val_loss: 0.3694 - val_accuracy: 0.8692\n"
     ]
    }
   ],
   "source": [
    "## use keras to load the dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "print(\"===================Shape of the Full Training and Test Set======================\")\n",
    "print(X_train_full.shape)\n",
    "print(y_train_full.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "## the dataset is already splitted to training and test set.\n",
    "## now the data is to be splitted to validation set.\n",
    "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "print(\"===================Shape of the Training and Test Set after the Splitting to Validation Set======================\")\n",
    "print(X_valid.shape)\n",
    "print(X_train.shape)\n",
    "print(y_valid.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "## for fashion mnist, the class names are\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]\n",
    "print(class_names)\n",
    "\n",
    "## first example in the trainning set \n",
    "print(y_train[2])\n",
    "print(class_names[y_train[0]])\n",
    "print(class_names[y_train[2]])\n",
    "\n",
    "print(\"==============Building the Model using Sequntial API=======================\")\n",
    "##defining the model\n",
    "## this is the line to reate the sequential model\n",
    "## simplest kind of keras model for neural networks, this is called sequential API.\n",
    "model = keras.models.Sequential()\n",
    "## First Layer is Flatten Layer\n",
    "## This will convert each input image to 1D array.\n",
    "## here we have to specify the input shape  = (28,28)\n",
    "## we dont need to give the batch size here. only the input shape.\n",
    "## alternatively we can give keras.layers.InputLayer(input_shape=[28,28])\n",
    "model.add(keras.layers.Flatten(input_shape = [28,28]))\n",
    "## next layer is dense layer with 300 neurons.\n",
    "## it will use ReLU activation function.\n",
    "model.add(keras.layers.Dense(300, activation = \"relu\"))\n",
    "## next layer is also a Dense layer with 100 neurons\n",
    "## it also uses the ReLU activation function.\n",
    "model.add(keras.layers.Dense(100, activation = \"relu\"))\n",
    "## next layer is the final layer with 10 neurons. to classify the 10 classes.\n",
    "## here using the softmax actiavtion function.\n",
    "model.add(keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "# ## we can do the same thing in the given below manner as well\n",
    "# model = keras.models.Sequential([\n",
    "#     keras.layers.Flatten(input_shape=[28,28]),\n",
    "#     keras.layers.Dense(300, activation = \"relu\"),\n",
    "#     keras.layers.Dense(100, activation = \"relu\"),\n",
    "#     keras.layers.Dense(10, activation = \"softmax\")\n",
    "# ])\n",
    "\n",
    "## printing the model summary\n",
    "print(model.summary())\n",
    "## print the model layers\n",
    "print(model.layers)\n",
    "## check the first hidden layer\n",
    "hidden1 = model.layers[1]\n",
    "print(hidden1.name)\n",
    "## checking the weights and biases.\n",
    "weights, biases = hidden1.get_weights()\n",
    "print(weights)\n",
    "print(weights.shape)\n",
    "print(biases)\n",
    "print(biases.shape)\n",
    "\n",
    "print(\"=================Compiling the model=====================\")\n",
    "## compile the model\n",
    "## compile is to specify the loss fnuction and optimizer to use while training and evaluating.\n",
    "## also we can give the metrics to use while training and evaluating model.\n",
    "## using loss = \"sparse_categorical_crossentropy\"\n",
    "## using optimizer as \"sgd\" which is stochcastic gradient descent.\n",
    "## using the metrics as \"accuracy\"\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "                 optimizer = \"sgd\",\n",
    "                 metrics = [\"accuracy\"])\n",
    "\n",
    "print(\"================Use Callbacks============================\")\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"fashion_mnist_classifier_model1.h5\", save_best_only=True)\n",
    "print(\"================Training the Model=======================\")\n",
    "history = model.fit(X_train, y_train, epochs = 10, \n",
    "                    validation_data = [X_valid, y_valid],\n",
    "                    callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EARLY STOPPING CONCEPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================Shape of the Full Training and Test Set======================\n",
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n",
      "===================Shape of the Training and Test Set after the Splitting to Validation Set======================\n",
      "(5000, 28, 28)\n",
      "(55000, 28, 28)\n",
      "(5000,)\n",
      "(55000,)\n",
      "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n",
      "7\n",
      "Coat\n",
      "Sneaker\n",
      "==============Building the Model using Sequntial API=======================\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[<keras.layers.reshaping.flatten.Flatten object at 0x0000022C2EA39E20>, <keras.layers.core.dense.Dense object at 0x0000022C2E7094F0>, <keras.layers.core.dense.Dense object at 0x0000022C2EA85550>, <keras.layers.core.dense.Dense object at 0x0000022C2916A6D0>]\n",
      "dense_15\n",
      "[[ 0.03725058 -0.0022904   0.06221877 ...  0.06228477 -0.01389557\n",
      "  -0.03631526]\n",
      " [ 0.06597993  0.02139638 -0.07039643 ...  0.01182574  0.0700274\n",
      "   0.01318623]\n",
      " [-0.04706573 -0.03557254 -0.03077848 ...  0.00129902  0.01079357\n",
      "  -0.00692589]\n",
      " ...\n",
      " [-0.04646512 -0.06428195  0.05184722 ...  0.03368853  0.02307725\n",
      "  -0.00323641]\n",
      " [ 0.02002923 -0.04106801  0.01774877 ... -0.03502107  0.00708999\n",
      "  -0.05117142]\n",
      " [ 0.05977792  0.01013071  0.0733507  ... -0.02315414  0.054636\n",
      "  -0.02241421]]\n",
      "(784, 300)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(300,)\n",
      "=================Compiling the model=====================\n",
      "================Use Callbacks============================\n",
      "================USING EARLY STOPPING METHODS===============\n",
      "================Training the Model=======================\n",
      "Epoch 1/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.7269 - accuracy: 0.7637 - val_loss: 0.4902 - val_accuracy: 0.8372\n",
      "Epoch 2/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4860 - accuracy: 0.8309 - val_loss: 0.4747 - val_accuracy: 0.8278\n",
      "Epoch 3/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4430 - accuracy: 0.8443 - val_loss: 0.4145 - val_accuracy: 0.8588\n",
      "Epoch 4/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4169 - accuracy: 0.8538 - val_loss: 0.3988 - val_accuracy: 0.8594\n",
      "Epoch 5/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3969 - accuracy: 0.8605 - val_loss: 0.3839 - val_accuracy: 0.8644\n",
      "Epoch 6/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3810 - accuracy: 0.8661 - val_loss: 0.3624 - val_accuracy: 0.8742\n",
      "Epoch 7/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3663 - accuracy: 0.8713 - val_loss: 0.3661 - val_accuracy: 0.8736\n",
      "Epoch 8/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3560 - accuracy: 0.8730 - val_loss: 0.3543 - val_accuracy: 0.8774\n",
      "Epoch 9/100\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3455 - accuracy: 0.8777 - val_loss: 0.3780 - val_accuracy: 0.8606\n",
      "Epoch 10/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3365 - accuracy: 0.8797 - val_loss: 0.3414 - val_accuracy: 0.8808\n",
      "Epoch 11/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3279 - accuracy: 0.8834 - val_loss: 0.3423 - val_accuracy: 0.8778\n",
      "Epoch 12/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3203 - accuracy: 0.8851 - val_loss: 0.3322 - val_accuracy: 0.8800\n",
      "Epoch 13/100\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3131 - accuracy: 0.8885 - val_loss: 0.3303 - val_accuracy: 0.8836\n",
      "Epoch 14/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3071 - accuracy: 0.8902 - val_loss: 0.3264 - val_accuracy: 0.8870\n",
      "Epoch 15/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2999 - accuracy: 0.8926 - val_loss: 0.3277 - val_accuracy: 0.8794\n",
      "Epoch 16/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2942 - accuracy: 0.8941 - val_loss: 0.3179 - val_accuracy: 0.8860\n",
      "Epoch 17/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2887 - accuracy: 0.8953 - val_loss: 0.3340 - val_accuracy: 0.8754\n",
      "Epoch 18/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2823 - accuracy: 0.8992 - val_loss: 0.3251 - val_accuracy: 0.8830\n",
      "Epoch 19/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2784 - accuracy: 0.8999 - val_loss: 0.3073 - val_accuracy: 0.8902\n",
      "Epoch 20/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2730 - accuracy: 0.9008 - val_loss: 0.3235 - val_accuracy: 0.8808\n",
      "Epoch 21/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2687 - accuracy: 0.9025 - val_loss: 0.3151 - val_accuracy: 0.8854\n",
      "Epoch 22/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2627 - accuracy: 0.9050 - val_loss: 0.3062 - val_accuracy: 0.8928\n",
      "Epoch 23/100\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2576 - accuracy: 0.9057 - val_loss: 0.3003 - val_accuracy: 0.8860\n",
      "Epoch 24/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2543 - accuracy: 0.9084 - val_loss: 0.3094 - val_accuracy: 0.8882\n",
      "Epoch 25/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2504 - accuracy: 0.9091 - val_loss: 0.2961 - val_accuracy: 0.8898\n",
      "Epoch 26/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2459 - accuracy: 0.9124 - val_loss: 0.3046 - val_accuracy: 0.8866\n",
      "Epoch 27/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2422 - accuracy: 0.9123 - val_loss: 0.2967 - val_accuracy: 0.8914\n",
      "Epoch 28/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2379 - accuracy: 0.9138 - val_loss: 0.2966 - val_accuracy: 0.8942\n",
      "Epoch 29/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2339 - accuracy: 0.9157 - val_loss: 0.2942 - val_accuracy: 0.8938\n",
      "Epoch 30/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2302 - accuracy: 0.9168 - val_loss: 0.2926 - val_accuracy: 0.8954\n",
      "Epoch 31/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2264 - accuracy: 0.9189 - val_loss: 0.3038 - val_accuracy: 0.8902\n",
      "Epoch 32/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2241 - accuracy: 0.9194 - val_loss: 0.2929 - val_accuracy: 0.8970\n",
      "Epoch 33/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2194 - accuracy: 0.9214 - val_loss: 0.2882 - val_accuracy: 0.8962\n",
      "Epoch 34/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2169 - accuracy: 0.9221 - val_loss: 0.2982 - val_accuracy: 0.8926\n",
      "Epoch 35/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2125 - accuracy: 0.9234 - val_loss: 0.2949 - val_accuracy: 0.8936\n",
      "Epoch 36/100\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2098 - accuracy: 0.9246 - val_loss: 0.2878 - val_accuracy: 0.8974\n",
      "Epoch 37/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2075 - accuracy: 0.9242 - val_loss: 0.2858 - val_accuracy: 0.8992\n",
      "Epoch 38/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2033 - accuracy: 0.9264 - val_loss: 0.3088 - val_accuracy: 0.8898\n",
      "Epoch 39/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2011 - accuracy: 0.9284 - val_loss: 0.2798 - val_accuracy: 0.8974\n",
      "Epoch 40/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1982 - accuracy: 0.9281 - val_loss: 0.2944 - val_accuracy: 0.8950\n",
      "Epoch 41/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1941 - accuracy: 0.9291 - val_loss: 0.2915 - val_accuracy: 0.8972\n",
      "Epoch 42/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1922 - accuracy: 0.9307 - val_loss: 0.2842 - val_accuracy: 0.8976\n",
      "Epoch 43/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1887 - accuracy: 0.9320 - val_loss: 0.3060 - val_accuracy: 0.8902\n",
      "Epoch 44/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1849 - accuracy: 0.9329 - val_loss: 0.3027 - val_accuracy: 0.8906\n",
      "Epoch 45/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1828 - accuracy: 0.9352 - val_loss: 0.2931 - val_accuracy: 0.8946\n",
      "Epoch 46/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1803 - accuracy: 0.9353 - val_loss: 0.2853 - val_accuracy: 0.8956\n",
      "Epoch 47/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1768 - accuracy: 0.9365 - val_loss: 0.2923 - val_accuracy: 0.8992\n",
      "Epoch 48/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1748 - accuracy: 0.9370 - val_loss: 0.2868 - val_accuracy: 0.8980\n",
      "Epoch 49/100\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1720 - accuracy: 0.9381 - val_loss: 0.2975 - val_accuracy: 0.8940\n"
     ]
    }
   ],
   "source": [
    "## use keras to load the dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "print(\"===================Shape of the Full Training and Test Set======================\")\n",
    "print(X_train_full.shape)\n",
    "print(y_train_full.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "## the dataset is already splitted to training and test set.\n",
    "## now the data is to be splitted to validation set.\n",
    "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "print(\"===================Shape of the Training and Test Set after the Splitting to Validation Set======================\")\n",
    "print(X_valid.shape)\n",
    "print(X_train.shape)\n",
    "print(y_valid.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "## for fashion mnist, the class names are\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]\n",
    "print(class_names)\n",
    "\n",
    "## first example in the trainning set \n",
    "print(y_train[2])\n",
    "print(class_names[y_train[0]])\n",
    "print(class_names[y_train[2]])\n",
    "\n",
    "print(\"==============Building the Model using Sequntial API=======================\")\n",
    "##defining the model\n",
    "## this is the line to reate the sequential model\n",
    "## simplest kind of keras model for neural networks, this is called sequential API.\n",
    "model = keras.models.Sequential()\n",
    "## First Layer is Flatten Layer\n",
    "## This will convert each input image to 1D array.\n",
    "## here we have to specify the input shape  = (28,28)\n",
    "## we dont need to give the batch size here. only the input shape.\n",
    "## alternatively we can give keras.layers.InputLayer(input_shape=[28,28])\n",
    "model.add(keras.layers.Flatten(input_shape = [28,28]))\n",
    "## next layer is dense layer with 300 neurons.\n",
    "## it will use ReLU activation function.\n",
    "model.add(keras.layers.Dense(300, activation = \"relu\"))\n",
    "## next layer is also a Dense layer with 100 neurons\n",
    "## it also uses the ReLU activation function.\n",
    "model.add(keras.layers.Dense(100, activation = \"relu\"))\n",
    "## next layer is the final layer with 10 neurons. to classify the 10 classes.\n",
    "## here using the softmax actiavtion function.\n",
    "model.add(keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "# ## we can do the same thing in the given below manner as well\n",
    "# model = keras.models.Sequential([\n",
    "#     keras.layers.Flatten(input_shape=[28,28]),\n",
    "#     keras.layers.Dense(300, activation = \"relu\"),\n",
    "#     keras.layers.Dense(100, activation = \"relu\"),\n",
    "#     keras.layers.Dense(10, activation = \"softmax\")\n",
    "# ])\n",
    "\n",
    "## printing the model summary\n",
    "print(model.summary())\n",
    "## print the model layers\n",
    "print(model.layers)\n",
    "## check the first hidden layer\n",
    "hidden1 = model.layers[1]\n",
    "print(hidden1.name)\n",
    "## checking the weights and biases.\n",
    "weights, biases = hidden1.get_weights()\n",
    "print(weights)\n",
    "print(weights.shape)\n",
    "print(biases)\n",
    "print(biases.shape)\n",
    "\n",
    "print(\"=================Compiling the model=====================\")\n",
    "## compile the model\n",
    "## compile is to specify the loss fnuction and optimizer to use while training and evaluating.\n",
    "## also we can give the metrics to use while training and evaluating model.\n",
    "## using loss = \"sparse_categorical_crossentropy\"\n",
    "## using optimizer as \"sgd\" which is stochcastic gradient descent.\n",
    "## using the metrics as \"accuracy\"\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "                 optimizer = \"sgd\",\n",
    "                 metrics = [\"accuracy\"])\n",
    "\n",
    "print(\"================Use Callbacks============================\")\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"fashion_mnist_classifier_model_earlystopping.h5\", save_best_only=True)\n",
    "print(\"================USING EARLY STOPPING METHODS===============\")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "print(\"================Training the Model=======================\")\n",
    "history = model.fit(X_train, y_train, epochs = 100, \n",
    "                    validation_data = [X_valid, y_valid],\n",
    "                    callbacks=[checkpoint_cb,early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model stop traininig at epoch 49."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
