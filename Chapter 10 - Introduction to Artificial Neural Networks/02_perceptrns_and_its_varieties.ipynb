{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERCEPTRON\n",
    "\n",
    "**PERCEPTRON** :- One of the Simplest Artificial Neural Network architecture which is Invented in 1957.\n",
    "\n",
    "+ It is based on threshold linear unit (TLU) or linear threshold unit (LTU)\n",
    "+ TLU will compute the weighted sum of the inputs.\n",
    "    $$Z = x1w1 + x2w2 + x3w3$$\n",
    "+ Then applies a Step Function to that sum and Outputs the Result.\n",
    "    $$hw(X) = step(Z)$$\n",
    "+ This is how the Basic Perceptron Works.\n",
    "\n",
    "+ Common step function used in Perceptrons\n",
    "\n",
    "    + Heaviside step function.\n",
    "    + Sign Function.\n",
    "    \n",
    "\n",
    "\n",
    "+ A single TLU can be used for linear binary classification.\n",
    "+ It computes a linear combination of inputs, and if the result exceeds a threshold.\n",
    "+ it output a positive class otherwise it outputs a negative class.\n",
    "+ Like Logistic Regression and SVMs.\n",
    "\n",
    "\n",
    "**PERCEPTRON ARCHITECTURE**\n",
    "\n",
    "+ Perceptron is Simply Composed of a Single Layer of TLUs.\n",
    "+ Each TLU connected to all the Inputs.\n",
    "+ When all the neurons in a layer are conected to every neurons in the previous layer, the layer is called fully conected layer. (Dense Layer)\n",
    "+ The input of the Perceptrons are passes through the Input Neurons.\n",
    "+ All the Input neurons forms input layer.\n",
    "+ Extra bias feature is generally added and it is called bias neuron, which output 1 all the time.\n",
    "+ The Decision Boundary of each output neuron is linear, so perceptrons are incapable of learning complex patterns.\n",
    "+ How ever if training insatnces are linearly seperable.\n",
    "+ it is demonstrated that this algorithm would converge to a solution. Which is called *Perceptron Convergence Theorem*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPLEMENTING PERCEPTRON USING SCIKIT LEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## implementing the perceptron using scikit learn.\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "## load the iris data\n",
    "iris = load_iris()\n",
    "iris.keys()\n",
    "X = iris.data\n",
    "X = X[:, (2,3)]   ## [[1.4, 0.2],, petal length and petal width\n",
    "y = iris.target\n",
    "y = (y==0).astype(np.int64)  ## Iris Setosa\n",
    "## define the perceptron\n",
    "perceptron_classifier = Perceptron()\n",
    "## fit the model\n",
    "perceptron_classifier.fit(X,y)\n",
    "## predict the model\n",
    "y_pred = perceptron_classifier.predict([[2, 0.5]])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above Perceptron Classifier is equivalent to the SGDClassifier with hyperparameter\n",
    "\n",
    "    + loss = \"perceptron\"\n",
    "    + learning_rate = \"constant\"\n",
    "    + eta0 = 1\n",
    "    + penalty = None (No regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to Logistic Regression Classifiers, Perceptron do not output a Class Probability, they make predictions based on a hard threshold. This one reason to prefer Logistic Regression over the Perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MULTILAYER PERCEPTRON AND BACKPROPOGATION\n",
    "\n",
    "+ An MLP is composed of \n",
    "\n",
    "\n",
    "        + one input layer\n",
    "        + one or more layers of TLU s (Hidded Layer)\n",
    "        + one final layer of TLU (Output layer)\n",
    "        + Layer close to the Input Layer : lower layer.\n",
    "        + Layer Close to the Output Layer : upper layer.\n",
    "        \n",
    "\n",
    "\n",
    "+ Every Layer excepts the Output layer includes a bias neuron and fully connected to the next layer.\n",
    "+ Hidden layer will be fully connected or Dense Layer.\n",
    "+ When an ANN contains a deep stack of hidden layers, it is called deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REGRESSION MLPs\n",
    "\n",
    "+ MLPs can be used for Regression Tasks.\n",
    "+ If you want to predict a Single Value, you just need single output neuron.\n",
    "+ For multivariate Regression, you will need to predict multiple values, so you will need multiple neurons.\n",
    "\n",
    "Generally when building MLP for Regression, you do not want to use any activation function for the Output neurons, So they are free to output any range of values.\n",
    "\n",
    "+ If we want positive output, the use the ReLU activation function in the output layer.\n",
    "+ or use softplus, variant of ReLU\n",
    "+ if we want a range bound output like between -1 and 1, then use hyperbolic tangent.\n",
    "+ if that is from 0 ot 1, then use logitsic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REGRSSION MLP ARCHITECTURE\n",
    "\n",
    "|Hyperparameter   |    Typical Value   |\n",
    "|-----------------|--------------------|\n",
    "| input neuron    |   One per input feature   |\n",
    "| hidden layer    |   Depends on the Problem typically 1 - 5 |\n",
    "| neurons per hidden layer |  Depends on the Problem typically 10 - 100  |\n",
    "| output neuron   |   per prediction and dimension  |\n",
    "| Hidden Activation |   ReLU  or SELU  |\n",
    "| Output Activation |   None, or ReLU/softplus for positive o/p and logistic/tanh - bounded o/p|\n",
    "|Loss Function |   MSE or MAE/Huber (if Outlier)  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLASSIFICATION MLPs \n",
    "\n",
    "+ MLPs can be used for Classification Tasks.\n",
    "+ For Binary Classification, we need single output neuron with Logistic Activation Function.\n",
    "+ For multilabel binary classification tasks, need two or more output neuron with logistic activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Hyperparameter |  Binary Classification  |  Multilabel Binary Classification  |  MultiCLass Classification  |\n",
    "|----------------|-------------------------| -----------------------------------| ----------------------------| \n",
    "| inputs and hidden layers | Same as Regression  |  Same as Regression  |  Same as Regression  | \n",
    "| output nerons |  1   |  1 per label  |  1 per class |\n",
    "| output layer activation |  Logistic  |  Logistic  | Softmax  |\n",
    "| Loss Function  |  Cross Entropy  |  Cross Entropy  |  Cross Entropy  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
