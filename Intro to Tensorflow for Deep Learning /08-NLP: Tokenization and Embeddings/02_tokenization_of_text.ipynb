{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlW+dnEOpmeeRScN0B3RZM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AMEERKOTTA/hands_on_machine_learning_with_sklearn_keras_and_tensorflow/blob/main/Intro%20to%20Tensorflow%20for%20Deep%20Learning%20/08-NLP%3A%20Tokenization%20and%20Embeddings/02_tokenization_of_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TOKENIZATION OF TEXT**\n",
        "\n",
        "Tokenization of Text is the Preparation of the Dataset to feed the Neural Network.\n",
        "\n",
        "There are three approaches.\n",
        "  + Turning Letters into Numbers.\n",
        "  + Turning Words into Numbers.\n",
        "  + Turning Sentences into Sequences.\n",
        "\n",
        "The drawback of turning the Letters into Numbers is that, if two words with same letters is happened to come accross, the model will interpret it as Same Word.\n",
        "\n",
        "It is better to Consider to turn Words into Numbers. While it is we have to Consider the Word Sequence. as it will give meaning to the Sentence. So Sentence to Sequence is a Better method to Tokenize the Text Data and Prepare for the Learning."
      ],
      "metadata": {
        "id": "8eUGHwy8zO0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TURNING WORDS INTO NUMBERS**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fyEk0H-E1PRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## code for turning words into numbers\n",
        "## import the tokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "## define the sentences\n",
        "sentences = [\n",
        "    \"My Favorite food is ice cream\",\n",
        "    \"do you like ice cream too?\"\n",
        "]\n",
        "## define teh tokenizer for max words = 10\n",
        "tokenizer = Tokenizer(num_words=10)\n",
        "## fit the tokenizer with the sentences.\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "## get the word index\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbX0lV_j1I2z",
        "outputId": "c5ad2bf3-0b4e-443f-bbb1-2dda39b1493e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ice': 1, 'cream': 2, 'my': 3, 'favorite': 4, 'food': 5, 'is': 6, 'do': 7, 'you': 8, 'like': 9, 'too': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WORDS SEQUENCE MATTERS**\n",
        "\n",
        "While turning the Words to Numbers, one important thing is to Consider the Word Sequence. Throwing words together in any old sequence can result in non-sense.\n",
        "\n",
        "Example :-\n",
        "  + chase dogs my squirrels to like\n",
        "  + dogs squirrels like my chase to\n",
        "\n",
        "+ use texts_to_sequence() to turn sentences to sequences.\n",
        "\n"
      ],
      "metadata": {
        "id": "lj2qSyc72Ooz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"My Favorite food is ice cream\",\n",
        "    \"Do you like ice cream too?\"\n",
        "]\n",
        "## turn into sequence\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbhaZ7Mu3PdZ",
        "outputId": "2b0fbdb2-af37-4b7a-c5f6-323c4ba99f05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3, 4, 5, 6, 1, 2], [7, 8, 9, 1, 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OOV TOKEN**\n",
        "\n",
        "+ Out of Vocabulary Tokens.\n",
        "+ What if the text being Sequenced contains words that are not in the Word index?\n",
        "+ Specify the Out of Vocabulary token to represent the words that are not in the Word index."
      ],
      "metadata": {
        "id": "JjBJkU2G3sU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"My favorite food is ice cream\",\n",
        "    \"Do you like ice cream too?\"\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 20, oov_token = \"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQP9InrK4YdK",
        "outputId": "ac663dc5-3e02-4131-9639-a29c0d3977c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'ice': 2, 'cream': 3, 'my': 4, 'favorite': 5, 'food': 6, 'is': 7, 'do': 8, 'you': 9, 'like': 10, 'too': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## new sentence.\n",
        "tokenizer = Tokenizer(num_words = 20, oov_token = \"<OOV>\")\n",
        "\n",
        "new_sentences = [\n",
        "    \"Your favorite food is strawberries and cream\"\n",
        "]\n",
        "\n",
        "new_sequences = tokenizer.texts_to_sequences(new_sentences)\n",
        "print(new_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CqxmOaE5CM-",
        "outputId": "cedc245b-009f-49d0-dcea-35639a664a63"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[None, None, None, None, None, None, None]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## import tokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "## define some sentences\n",
        "sentences = [\n",
        "    'My favorite food is ice cream',\n",
        "    'do you like ice cream too?',\n",
        "    'My dog likes ice cream!',\n",
        "    \"your favorite flavor of icecream is chocolate\",\n",
        "    \"chocolate isn't good for dogs\",\n",
        "    \"your dog, your cat, and your parrot prefer broccoli\"\n",
        "]\n",
        "\n",
        "# Optionally set the max number of words to tokenize.\n",
        "# The out of vocabulary (OOV) token represents words that are not in the index.\n",
        "# Call fit_on_text() on the tokenizer to generate unique numbers for each word\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)\n",
        "# Get the number for a given word\n",
        "print(\"Word Index of Favorite :-\",word_index['favorite'])\n",
        "\n",
        "## create sequences for the sentences\n",
        "## After you tokenize the words, the word index contains a unique number for each word. \n",
        "## However, the numbers in the word index are not ordered. \n",
        "## Words in a sentence have an order. \n",
        "## So after tokenizing the words, the next step is to generate sequences for the sentences.\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print (\"Sequence of the Sentences :-\",sequences)\n",
        "\n",
        "## The Out of Vocabluary (OOV) token is the first entry in the word index. \n",
        "## You will see it shows up in the sequences in place of any word that is not in the word index.\n",
        "sentences2 = [\"I like hot chocolate\", \n",
        "              \"My dogs and my hedgehog like kibble but my squirrel prefers grapes and my chickens like ice cream, preferably vanilla\"]\n",
        "\n",
        "sequences2 = tokenizer.texts_to_sequences(sentences2)\n",
        "print(sequences2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBBTnZAOzguP",
        "outputId": "235d4aaf-c4f6-4a45-8b3f-d0a1e601194f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'your': 2, 'ice': 3, 'cream': 4, 'my': 5, 'favorite': 6, 'is': 7, 'dog': 8, 'chocolate': 9, 'food': 10, 'do': 11, 'you': 12, 'like': 13, 'too': 14, 'likes': 15, 'flavor': 16, 'of': 17, 'icecream': 18, \"isn't\": 19, 'good': 20, 'for': 21, 'dogs': 22, 'cat': 23, 'and': 24, 'parrot': 25, 'prefer': 26, 'broccoli': 27}\n",
            "Word Index of Favorite :- 6\n",
            "Sequence of the Sentences :- [[5, 6, 10, 7, 3, 4], [11, 12, 13, 3, 4, 14], [5, 8, 15, 3, 4], [2, 6, 16, 17, 18, 7, 9], [9, 19, 20, 21, 22], [2, 8, 2, 23, 24, 2, 25, 26, 27]]\n",
            "[[1, 13, 1, 9], [5, 22, 24, 5, 1, 13, 1, 1, 5, 1, 1, 1, 24, 5, 1, 13, 3, 4, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why we need OOV tokens :\n",
        "\n",
        "+ when we train the model or neural network.\n",
        "+ you train the model with known dataset.\n",
        "+ and validate and predict using the dataset that are not seen yet.\n",
        "+ so it is very likely that, validation and prediction text will contain words that are not in the word index.\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "vsG3owkx8ait"
      }
    }
  ]
}